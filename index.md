<link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>

<p align="justify" style="padding-top: 10px; font-family:'Roboto'; font-size:18px;" >
We introduce the dynamic grasp synthesis task: given an object with a known 6D pose and a grasp reference, our goal is 
to generate motions that move the object to a target 6D pose. This is challenging, because it requires reasoning about 
the complex articulation of the human hand and the intricate physical interaction with the object. We propose a novel 
method that frames this problem in the reinforcement learning framework and leverages a physics simulation, both to 
learn and to evaluate such dynamic interactions. A hierarchical approach decomposes the task into low-level grasping 
and high-level motion synthesis. It can be used to generate novel hand sequences that approach, grasp, and move an 
object to a desired location, while retaining human-likeness. We show that our approach leads to stable grasps and 
generates a wide range of motions. In the motion synthesis policy, we learn to move the object to 6D target poses, 
while maintaining a stable grasp where the object does not slip out of the hand. Furthermore, even imperfect labels 
can be corrected by our method to generate dynamic interaction sequences. 
</p>


[comment]: <> (<div class="video_div">)

[comment]: <> (<h1 align="center">Video</h1>)

[comment]: <> (<iframe class="video_player" width="960" height="540" src="https://www.youtube.com/embed/lZBl8_6y5QU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>)

[comment]: <> (</div>)


[comment]: <> (<h1 align="center">Poster</h1>)

[comment]: <> (<script src="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.10.0/js/lightbox-plus-jquery.js"></script>)

[comment]: <> (<link href="https://cdnjs.cloudflare.com/ajax/libs/lightbox2/2.10.0/css/lightbox.min.css" rel="stylesheet" />)

[comment]: <> (<a data-lightbox="image-1" data-title="Poster" href="img/cose_png.png"><img src="img/cose_png.png"/></a>)

[comment]: <> (&#40;Click poster to view full size&#41;)

[comment]: <> (<h1 align="center">Completion Results</h1>)

[comment]: <> (<p align="justify" style="padding-top: 0px; font-family:'Roboto';" >)

[comment]: <> (Flowchart samples generated by our model. Given a number of strokes drawn by the users &#40;in black color&#41;, our model autogressively completes the drawing stroke-by-stroke. )

[comment]: <> (At every prediction step, the generated strokes are illustrated with different colors. )

[comment]: <> (</p>)

[comment]: <> (<p align="justify" style="padding-top: 0px; font-family:'Roboto'; " >)

[comment]: <> (In the accompanying figures &#40;right or below&#41;, we visualize the attention weights during the prediction of a particular stroke &#40;in red color&#41;.)

[comment]: <> (Darker blue corresponds to larger attention weights. )

[comment]: <> (</p>)


[comment]: <> (![Image]&#40;img/data_1_pos_and_attention.gif&#41;)

[comment]: <> (---)

[comment]: <> (![Image]&#40;img/data_164_pos_and_attention.gif&#41;)

[comment]: <> (---)

[comment]: <> (![Image]&#40;img/data_41_pos_and_attention.gif&#41;)

[comment]: <> (---)

[comment]: <> (<center><img style="width:40%" src="img/data_10_pos_and_attention.gif"></center>)

[comment]: <> (---)

[comment]: <> (![Image]&#40;img/data_170_given2_pos_and_attention.gif&#41;)

[comment]: <> (---)

[comment]: <> (![Image]&#40;img/data_170_given3_pos_and_attention.gif&#41;)

[comment]: <> (---)

[comment]: <> (![Image]&#40;img/data_189_given10_pos_and_attention.gif&#41;)
